{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--WpBgpTYvhK"
      },
      "source": [
        "# Introduction #\n",
        "This code has the following functions:\n",
        "1. A new version of deep convolutional neural network model (ResNet) for binary classfication based on the base project COVID-CXR\n",
        "2. Generate a h5 model file with DNN_RESNET model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEmA6MhtBuym"
      },
      "source": [
        "# Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Pg59NHt4A-p"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import yaml\n",
        "import os\n",
        "import datetime\n",
        "import random\n",
        "import cv2\n",
        "import dill\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.summary as tf_summary\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from math import ceil\n",
        "from tensorflow.keras.metrics import BinaryAccuracy, CategoricalAccuracy, Precision, Recall, AUC\n",
        "from tensorflow.keras.models import save_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "\n",
        "from tensorflow.keras import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input, MaxPool2D, Conv2D, Flatten, LeakyReLU, BatchNormalization, Activation, concatenate, GlobalAveragePooling2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from tensorflow.keras.applications.resnet_v2 import ResNet50V2, ResNet101V2\n",
        "\n",
        "from tensorflow.keras.metrics import Metric\n",
        "from tensorflow.python.keras.utils import metrics_utils\n",
        "from tensorflow.python.ops import init_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.keras.utils.generic_utils import to_list\n",
        "from tensorflow.python.keras import backend as K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxPchNkeZSX2"
      },
      "source": [
        "# Define functions that will be used in the main program\n",
        "This following cell was a refined version in order to run on Google colab. \n",
        "\n",
        "Major adjustments I did for these code comparing with the orignial code is:\n",
        "\n",
        "1. Adjust the code to run on Mac and Ubuntu since the original code is developed under Windows and its style for directory creates difficulty to run in other Operation Systems;\n",
        "2. Adjust the code to run with the new version of Tensorflow. The main change is replace fit_generator(...) with fit(...) because fit_generator(...) was depreciated by the new version;\n",
        "3. Fix other mistakes when migrate to run on Google Colab. (The original code was run on command line)\n",
        "\n",
        "The original code can be found by the following link: https://github.com/aildnont/covid-cxr.\n",
        "* The following cell contains useful functions from the above link to generate the model file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvUh7r7H3E_Q"
      },
      "outputs": [],
      "source": [
        "class F1Score(Metric):\n",
        "    '''\n",
        "    Custom tf.keras metric that calculates the F1 Score\n",
        "    '''\n",
        "\n",
        "    def __init__(self, thresholds=None, top_k=None, class_id=None, name=None, dtype=None):\n",
        "        '''\n",
        "        Creates an instance of the  F1Score class\n",
        "        :param thresholds: A float value or a python list/tuple of float threshold values in [0, 1].\n",
        "        :param top_k: An int value specifying the top-k predictions to consider when calculating precision\n",
        "        :param class_id: Integer class ID for which we want binary metrics. This must be in the half-open interval\n",
        "                `[0, num_classes)`, where `num_classes` is the last dimension of predictions\n",
        "        :param name: string name of the metric instance\n",
        "        :param dtype: data type of the metric result\n",
        "        '''\n",
        "        super(F1Score, self).__init__(name=name, dtype=dtype)\n",
        "        self.init_thresholds = thresholds\n",
        "        self.top_k = top_k\n",
        "        self.class_id = class_id\n",
        "\n",
        "        default_threshold = 0.5 if top_k is None else metrics_utils.NEG_INF\n",
        "        self.thresholds = metrics_utils.parse_init_thresholds(\n",
        "            thresholds, default_threshold=default_threshold)\n",
        "        self.true_positives = self.add_weight('true_positives', shape=(len(self.thresholds),),\n",
        "                                              initializer=init_ops.zeros_initializer)\n",
        "        self.false_positives = self.add_weight('false_positives', shape=(len(self.thresholds),),\n",
        "                                               initializer=init_ops.zeros_initializer)\n",
        "        self.false_negatives = self.add_weight('false_negatives', shape=(len(self.thresholds),),\n",
        "                                               initializer=init_ops.zeros_initializer)\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        '''\n",
        "        Accumulates true positive, false positive and false negative statistics.\n",
        "        :param y_true: The ground truth values, with the same dimensions as `y_pred`. Will be cast to `bool`\n",
        "        :param y_pred: The predicted values. Each element must be in the range `[0, 1]`\n",
        "        :param sample_weight: Weighting of each example. Defaults to 1. Can be a `Tensor` whose rank is either 0,\n",
        "               or the same rank as `y_true`, and must be broadcastable to `y_true`\n",
        "        :return: Update operation\n",
        "        '''\n",
        "        metrics_utils.update_confusion_matrix_variables(\n",
        "            {\n",
        "                metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,\n",
        "                metrics_utils.ConfusionMatrix.FALSE_POSITIVES: self.false_positives,\n",
        "                metrics_utils.ConfusionMatrix.FALSE_NEGATIVES: self.false_negatives\n",
        "            },\n",
        "            y_true, y_pred, thresholds=self.thresholds, top_k=self.top_k, class_id=self.class_id,\n",
        "            sample_weight=sample_weight)\n",
        "\n",
        "\n",
        "    def result(self):\n",
        "        '''\n",
        "        Compute the value for the F1 score. Calculates precision and recall, then F1 score.\n",
        "        F1 = 2 * precision * recall / (precision + recall)\n",
        "        :return: F1 score\n",
        "        '''\n",
        "        precision = math_ops.div_no_nan(self.true_positives, self.true_positives + self.false_positives)\n",
        "        recall = math_ops.div_no_nan(self.true_positives, self.true_positives + self.false_negatives)\n",
        "        result = math_ops.div_no_nan(2 * precision * recall, precision + recall)\n",
        "        return result[0] if len(self.thresholds) == 1 else result\n",
        "\n",
        "    def reset_states(self):\n",
        "        '''\n",
        "        Resets all of the metric state variables. Called between epochs, when a metric is evaluated during training.\n",
        "        '''\n",
        "        num_thresholds = len(to_list(self.thresholds))\n",
        "        K.batch_set_value(\n",
        "            [(v, np.zeros((num_thresholds,))) for v in self.variables])\n",
        "\n",
        "    def get_config(self):\n",
        "        '''\n",
        "        Returns the serializable config of the metric.\n",
        "        :return: serializable config of the metric\n",
        "        '''\n",
        "        config = {\n",
        "            'thresholds': self.init_thresholds,\n",
        "            'top_k': self.top_k,\n",
        "            'class_id': self.class_id\n",
        "        }\n",
        "        base_config = super(F1Score, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "\n",
        "def remove_text(img):\n",
        "    '''\n",
        "    Attempts to remove bright textual artifacts from X-ray images. For example, many images indicate the right side of\n",
        "    the body with a white 'R'. Works only for very bright text.\n",
        "    :param img: Numpy array of image\n",
        "    :return: Array of image with (ideally) any characters removed and inpainted\n",
        "    '''\n",
        "    mask = cv2.threshold(img, 230, 255, cv2.THRESH_BINARY)[1][:, :, 0].astype(np.uint8)\n",
        "    img = img.astype(np.uint8)\n",
        "    result = cv2.inpaint(img, mask, 10, cv2.INPAINT_NS).astype(np.float32)\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_class_weights(histogram, class_multiplier=None):\n",
        "    '''\n",
        "    Computes weights for each class to be applied in the loss function during training.\n",
        "    :param histogram: A list depicting the number of each item in different class\n",
        "    :param class_multiplier: List of values to multiply the calculated class weights by. For further control of class weighting.\n",
        "    :return: A dictionary containing weights for each class\n",
        "    '''\n",
        "    weights = [None] * len(histogram)\n",
        "    for i in range(len(histogram)):\n",
        "        weights[i] = (1.0 / len(histogram)) * sum(histogram) / histogram[i]\n",
        "    class_weight = {i: weights[i] for i in range(len(histogram))}\n",
        "    if class_multiplier is not None:\n",
        "        class_weight = [class_weight[i] * class_multiplier[i] for i in range(len(histogram))]\n",
        "    print(\"Class weights: \", class_weight)\n",
        "    return class_weight\n",
        "\n",
        "\n",
        "def random_minority_oversample(train_set):\n",
        "    '''\n",
        "    Oversample the minority class using the specified algorithm\n",
        "    :param train_set: Training set image file names and labels\n",
        "    :return: A new training set containing oversampled examples\n",
        "    '''\n",
        "    X_train = train_set[[x for x in train_set.columns if x != 'label']].to_numpy()\n",
        "    if X_train.shape[1] == 1:\n",
        "        X_train = np.expand_dims(X_train, axis=-1)\n",
        "    Y_train = train_set['label'].to_numpy()\n",
        "    sampler = RandomOverSampler(random_state=np.random.randint(0, high=1000))\n",
        "    X_resampled, Y_resampled = sampler.fit_resample(X_train, Y_train)\n",
        "    filenames = X_resampled[:, 1]     # Filename is in second column\n",
        "    label_strs = X_resampled[:, 2]    # Class name is in second column\n",
        "    print(\"Train set shape before oversampling: \", X_train.shape, \" Train set shape after resampling: \", X_resampled.shape)\n",
        "    train_set_resampled = pd.DataFrame({'filename': filenames, 'label': Y_resampled, 'label_str': label_strs})\n",
        "    return train_set_resampled\n",
        "\n",
        "\n",
        "def train_model(cfg, data, callbacks, verbose=1):\n",
        "    '''\n",
        "    Train a and evaluate model on given data.\n",
        "    :param cfg: Project config (from config.yml)\n",
        "    :param data: dict of partitioned dataset\n",
        "    :param callbacks: list of callbacks for Keras model\n",
        "    :param verbose: Verbosity mode to pass to model.fit_generator()\n",
        "    :return: Trained model and associated performance metrics on the test set\n",
        "    '''\n",
        "\n",
        "    # Create ImageDataGenerators\n",
        "    train_img_gen = ImageDataGenerator(rotation_range=10, preprocessing_function=remove_text,\n",
        "                                       samplewise_std_normalization=True, samplewise_center=True)\n",
        "    val_img_gen = ImageDataGenerator(preprocessing_function=remove_text,\n",
        "                                       samplewise_std_normalization=True, samplewise_center=True)\n",
        "    test_img_gen = ImageDataGenerator(preprocessing_function=remove_text,\n",
        "                                       samplewise_std_normalization=True, samplewise_center=True)\n",
        "\n",
        "    # Create DataFrameIterators\n",
        "    img_shape = tuple([224, 224])\n",
        "    y_col = 'label_str'\n",
        "    class_mode = 'categorical'\n",
        "    train_generator = train_img_gen.flow_from_dataframe(dataframe=data['TRAIN'], directory=covid_cxr + cfg['PATHS']['RAW_DATA'],\n",
        "        x_col=\"filename\", y_col=y_col, target_size=img_shape, batch_size=cfg['TRAIN']['BATCH_SIZE'],\n",
        "        class_mode=class_mode, validate_filenames=False)\n",
        "    val_generator = val_img_gen.flow_from_dataframe(dataframe=data['VAL'], directory=covid_cxr + cfg['PATHS']['RAW_DATA'],\n",
        "        x_col=\"filename\", y_col=y_col, target_size=img_shape, batch_size=cfg['TRAIN']['BATCH_SIZE'],\n",
        "        class_mode=class_mode, validate_filenames=False)\n",
        "    test_generator = test_img_gen.flow_from_dataframe(dataframe=data['TEST'], directory=covid_cxr + cfg['PATHS']['RAW_DATA'],\n",
        "        x_col=\"filename\", y_col=y_col, target_size=img_shape, batch_size=cfg['TRAIN']['BATCH_SIZE'],\n",
        "        class_mode=class_mode, validate_filenames=False, shuffle=False)\n",
        "\n",
        "    # Apply class imbalance strategy. We have many more X-rays negative for COVID-19 than positive.\n",
        "    histogram = np.bincount(np.array(train_generator.labels).astype(int))  # Get class distribution\n",
        "    class_weight = None\n",
        "    if cfg['TRAIN']['IMB_STRATEGY'] == 'class_weight':\n",
        "        class_multiplier = cfg['TRAIN']['CLASS_MULTIPLIER']\n",
        "        class_multiplier = [class_multiplier[cfg['DATA']['CLASSES'].index(c)] for c in test_generator.class_indices]\n",
        "        class_weight = get_class_weights(histogram, class_multiplier)\n",
        "\n",
        "    # Define metrics.\n",
        "    covid_class_idx = test_generator.class_indices['COVID-19']   # Get index of COVID-19 class\n",
        "    thresholds = 1.0 / len(cfg['DATA']['CLASSES'])      # Binary classification threshold for a class\n",
        "    metrics = [CategoricalAccuracy(name='accuracy'),\n",
        "               Precision(name='precision', thresholds=thresholds, class_id=covid_class_idx),\n",
        "               Recall(name='recall', thresholds=thresholds, class_id=covid_class_idx),\n",
        "               AUC(name='auc'),\n",
        "               F1Score(name='f1score', thresholds=thresholds, class_id=covid_class_idx)]\n",
        "\n",
        "    # Define the model.\n",
        "    print('Training distribution: ', ['Class ' + list(test_generator.class_indices.keys())[i] + ': ' + str(histogram[i]) + '. '\n",
        "           for i in range(len(histogram))])\n",
        "    input_shape = cfg['DATA']['IMG_DIM'] + [3]\n",
        "\n",
        "    histogram = np.bincount(data['TRAIN']['label'].astype(int))\n",
        "    output_bias = np.log([histogram[i] / (np.sum(histogram) - histogram[i]) for i in range(histogram.shape[0])])\n",
        "    model = dcnn_resnet_binary(input_shape, metrics, output_bias=output_bias)\n",
        "\n",
        "    # Train the model.\n",
        "    steps_per_epoch = ceil(train_generator.n / train_generator.batch_size)\n",
        "    val_steps = ceil(val_generator.n / val_generator.batch_size)\n",
        "    history = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=cfg['TRAIN']['EPOCHS'],\n",
        "                                  validation_data=val_generator, validation_steps=val_steps, callbacks=callbacks,\n",
        "                                  verbose=verbose)\n",
        "    # Run the model on the test set and print the resulting performance metrics.\n",
        "    test_results = model.evaluate_generator(test_generator, verbose=1)\n",
        "    test_metrics = {}\n",
        "    test_summary_str = [['**Metric**', '**Value**']]\n",
        "    for metric, value in zip(model.metrics_names, test_results):\n",
        "        test_metrics[metric] = value\n",
        "        print(metric, ' = ', value)\n",
        "        test_summary_str.append([metric, str(value)])\n",
        "    return model, test_metrics, test_generator\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38MT-5fUCC6U"
      },
      "source": [
        "# Define my own version of of DCNN_RESNET. #\n",
        "I defined a function dcnn_resnet_binary based on the original code. The main change is I used some new hyperparameters. \n",
        "\n",
        "It is not difficult to create more models like ResNet50V2, ResNet101V2, VGG16, and etc. All of them can generate different h5 model files for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAY_NZWNCBL8"
      },
      "outputs": [],
      "source": [
        "def dcnn_resnet_binary(input_shape, metrics, output_bias=None):\n",
        "    '''\n",
        "    Defines a deep convolutional neural network model for binary X-ray classification.\n",
        "    :param model_config: A dictionary of parameters associated with the model architecture\n",
        "    :param input_shape: The shape of the model input\n",
        "    :param metrics: Metrics to track model's performance\n",
        "    :return: a Keras Model object with the architecture defined in this method\n",
        "    '''\n",
        "\n",
        "    # Set hyperparameters\n",
        "    nodes_dense0 = 128\n",
        "    lr = 0.00001\n",
        "    dropout = 0.4\n",
        "    l2_lambda = 0.0001\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    \n",
        "    init_filters = 16\n",
        "    filter_exp_base = 3\n",
        "    conv_blocks = 3\n",
        "    kernel_size = (3,3)\n",
        "    max_pool_size = (2,2)\n",
        "    strides = (1,1)\n",
        "\n",
        "    # Set output bias\n",
        "    if output_bias is not None:\n",
        "        output_bias = Constant(output_bias)\n",
        "\n",
        "    # Input layer\n",
        "    X_input = Input(input_shape)\n",
        "    X = X_input\n",
        "\n",
        "    # Add convolutional (residual) blocks\n",
        "    for i in range(conv_blocks):\n",
        "        X_res = X\n",
        "        X = Conv2D(init_filters * (filter_exp_base ** i), kernel_size, strides=strides, padding='same',\n",
        "                         kernel_initializer='he_uniform', activity_regularizer=l2(l2_lambda),\n",
        "                         name='conv' + str(i) + '_0')(X)\n",
        "        X = BatchNormalization()(X)\n",
        "        X = LeakyReLU()(X)\n",
        "        X = Conv2D(init_filters * (filter_exp_base ** i), kernel_size, strides=strides, padding='same',\n",
        "                         kernel_initializer='he_uniform', activity_regularizer=l2(l2_lambda),\n",
        "                         name='conv' + str(i) + '_1')(X)\n",
        "        X = concatenate([X, X_res], name='concat' + str(i))\n",
        "        X = BatchNormalization()(X)\n",
        "        X = LeakyReLU()(X)\n",
        "        X = MaxPool2D(max_pool_size, padding='same')(X)\n",
        "\n",
        "    # Add fully connected layers\n",
        "    X = Flatten()(X)\n",
        "    X = Dropout(dropout)(X)\n",
        "    X = Dense(nodes_dense0, kernel_initializer='he_uniform', activity_regularizer=l2(l2_lambda))(X)\n",
        "    X = LeakyReLU()(X)\n",
        "    X = Dense(2, bias_initializer=output_bias)(X)\n",
        "    Y = Activation('softmax', dtype='float32', name='output')(X)\n",
        "\n",
        "    # Set model loss function, optimizer, metrics.\n",
        "    model = Model(inputs=X_input, outputs=Y)\n",
        "    model.summary()\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=metrics)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_x5uTExaA5j"
      },
      "source": [
        "# Preparing updated covid-cxr #\n",
        "\n",
        "This project is based on the previous work from https://github.com/aildnont/covid-cxr. However, the original code is developed under windows which do not work on a Mac. It also cannot run in the python environment other than version 3.6. It only works with tensorflow_gpu and etc.\n",
        "\n",
        "I fixed most of the issues and did the following work for our project:\n",
        "1. Preprocess the data and generate train, test, validation, and store this information in three separate files (covid-cxr/data/processed/train_set.csv, covid-cxr/data/processed/test_set.csv, covid-cxr/data/processed/val_set.csv);\n",
        "2. Modify the config.yml which defines the parameters;\n",
        "3. Store all the images files which used to predict;\n",
        "4. Generate the model files and store them in the following directory (covid-cxr/results/models).\n",
        "\n",
        "\n",
        "To make it easy, I shared all codes, data, and models with the public, and you can access them through the below link.\n",
        "https://drive.google.com/drive/folders/1wyvVPx9TvArGIldJkDpQfLRufwKZOTKQ?usp=sharing\n",
        "\n",
        "You have two ways to use it:\n",
        "1. The fast way to use it is as below:\n",
        "Step 1. Login to your google drive and paste the above link to your browser;\n",
        "Step 2. Right-click the folder name *covid-cxr* and click *Add shortcut to Drive*. Now, if you connect your google drive in colab, you will see the covid-cxr directory has been linked in your home.\n",
        "\n",
        "The code connects your google drive in colab environment:\n",
        "\n",
        "```\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "```\n",
        "\n",
        "2. You can also choose to download the whole directory as a zip file, and upload it to your google colab directory. Since it is huge (3GB), it could be slow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAzMhj_XajfK"
      },
      "source": [
        "#Load model and config file#\n",
        "In order to reproduce the result, you will need to use your own directory to load model and data. Please make sure you finish what I describe in the Preparing updated covid-cxr section before run these codes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMMLOsJk3sgp",
        "outputId": "2080f04f-98bd-4c24-9c14-0ab43898aecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Load gdrive since all data is stored on gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "covid_cxr = '/content/drive/MyDrive/covid-cxr/'\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "cfg = yaml.safe_load(open(Path(covid_cxr) / 'config.yml', 'r'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maV87zcoapNZ"
      },
      "source": [
        "# Load test set, callback and generate h5 file\n",
        "\n",
        "We collected a very large dataset containing chest x-ray image files. To balance accuracy and time to run it, we set Epoch to 5.\n",
        "\n",
        "When the training finished, a file named \"covid.h5\" will be generated. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80bCkkz93JbA",
        "outputId": "40d6caa6-bdaf-40b2-e4ec-a31ca6eb31a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1489 non-validated image filenames belonging to 2 classes.\n",
            "Found 146 non-validated image filenames belonging to 2 classes.\n",
            "Found 192 non-validated image filenames belonging to 2 classes.\n",
            "Class weights:  [26.589285714285715, 0.07643737166324435]\n",
            "Training distribution:  ['Class COVID-19: 28. ', 'Class non-COVID-19: 1461. ']\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv0_0 (Conv2D)               (None, 224, 224, 16  448         ['input_2[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 224, 224, 16  64         ['conv0_0[0][0]']                \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " leaky_re_lu_7 (LeakyReLU)      (None, 224, 224, 16  0           ['batch_normalization_6[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv0_1 (Conv2D)               (None, 224, 224, 16  2320        ['leaky_re_lu_7[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " concat0 (Concatenate)          (None, 224, 224, 19  0           ['conv0_1[0][0]',                \n",
            "                                )                                 'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 224, 224, 19  76         ['concat0[0][0]']                \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " leaky_re_lu_8 (LeakyReLU)      (None, 224, 224, 19  0           ['batch_normalization_7[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPooling2D)  (None, 112, 112, 19  0          ['leaky_re_lu_8[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv1_0 (Conv2D)               (None, 112, 112, 48  8256        ['max_pooling2d_3[0][0]']        \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 112, 112, 48  192        ['conv1_0[0][0]']                \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " leaky_re_lu_9 (LeakyReLU)      (None, 112, 112, 48  0           ['batch_normalization_8[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv1_1 (Conv2D)               (None, 112, 112, 48  20784       ['leaky_re_lu_9[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " concat1 (Concatenate)          (None, 112, 112, 67  0           ['conv1_1[0][0]',                \n",
            "                                )                                 'max_pooling2d_3[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 112, 112, 67  268        ['concat1[0][0]']                \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " leaky_re_lu_10 (LeakyReLU)     (None, 112, 112, 67  0           ['batch_normalization_9[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_4 (MaxPooling2D)  (None, 56, 56, 67)  0           ['leaky_re_lu_10[0][0]']         \n",
            "                                                                                                  \n",
            " conv2_0 (Conv2D)               (None, 56, 56, 144)  86976       ['max_pooling2d_4[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 56, 56, 144)  576        ['conv2_0[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " leaky_re_lu_11 (LeakyReLU)     (None, 56, 56, 144)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " conv2_1 (Conv2D)               (None, 56, 56, 144)  186768      ['leaky_re_lu_11[0][0]']         \n",
            "                                                                                                  \n",
            " concat2 (Concatenate)          (None, 56, 56, 211)  0           ['conv2_1[0][0]',                \n",
            "                                                                  'max_pooling2d_4[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 56, 56, 211)  844        ['concat2[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " leaky_re_lu_12 (LeakyReLU)     (None, 56, 56, 211)  0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_5 (MaxPooling2D)  (None, 28, 28, 211)  0          ['leaky_re_lu_12[0][0]']         \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 165424)       0           ['max_pooling2d_5[0][0]']        \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 165424)       0           ['flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 128)          21174400    ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " leaky_re_lu_13 (LeakyReLU)     (None, 128)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 2)            258         ['leaky_re_lu_13[0][0]']         \n",
            "                                                                                                  \n",
            " output (Activation)            (None, 2)            0           ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 21,482,230\n",
            "Trainable params: 21,481,220\n",
            "Non-trainable params: 1,010\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5\n",
            "47/47 [==============================] - ETA: 0s - loss: 451.0175 - accuracy: 0.9476 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9610 - f1score: 0.0000e+00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2034: UserWarning: Metric F1Score implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r47/47 [==============================] - 144s 3s/step - loss: 451.0175 - accuracy: 0.9476 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.9610 - f1score: 0.0000e+00 - val_loss: 358.3711 - val_accuracy: 0.8699 - val_precision: 0.0556 - val_recall: 0.3333 - val_auc: 0.9105 - val_f1score: 0.0952\n",
            "Epoch 2/5\n",
            "47/47 [==============================] - 81s 2s/step - loss: 419.7025 - accuracy: 0.9805 - precision: 0.4286 - recall: 0.1071 - auc: 0.9868 - f1score: 0.1714 - val_loss: 305.6641 - val_accuracy: 0.5753 - val_precision: 0.0317 - val_recall: 0.6667 - val_auc: 0.6435 - val_f1score: 0.0606\n",
            "Epoch 3/5\n",
            "47/47 [==============================] - 81s 2s/step - loss: 395.2088 - accuracy: 0.9738 - precision: 0.1765 - recall: 0.1071 - auc: 0.9885 - f1score: 0.1333 - val_loss: 291.6352 - val_accuracy: 0.8219 - val_precision: 0.0400 - val_recall: 0.3333 - val_auc: 0.9144 - val_f1score: 0.0714\n",
            "Epoch 4/5\n",
            "47/47 [==============================] - 81s 2s/step - loss: 374.3308 - accuracy: 0.9738 - precision: 0.2381 - recall: 0.1786 - auc: 0.9887 - f1score: 0.2041 - val_loss: 290.1660 - val_accuracy: 0.9795 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9924 - val_f1score: 0.0000e+00\n",
            "Epoch 5/5\n",
            "47/47 [==============================] - 81s 2s/step - loss: 356.4850 - accuracy: 0.9785 - precision: 0.4000 - recall: 0.2857 - auc: 0.9902 - f1score: 0.3333 - val_loss: 292.9459 - val_accuracy: 0.9247 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9760 - val_f1score: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:198: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 67s 13s/step - loss: 294.5326 - accuracy: 0.9375 - precision: 0.4000 - recall: 0.6667 - auc: 0.9772 - f1score: 0.5000\n",
            "loss  =  294.5326232910156\n",
            "accuracy  =  0.9375\n",
            "precision  =  0.4000000059604645\n",
            "recall  =  0.6666666865348816\n",
            "auc  =  0.9772406220436096\n",
            "f1score  =  0.5\n"
          ]
        }
      ],
      "source": [
        "# Load dataset file paths and labels\n",
        "data = {}\n",
        "data['TRAIN'] = pd.read_csv(covid_cxr + cfg['PATHS']['TRAIN_SET'])\n",
        "data['VAL'] = pd.read_csv(covid_cxr + cfg['PATHS']['VAL_SET'])\n",
        "data['TEST'] = pd.read_csv(covid_cxr + cfg['PATHS']['TEST_SET'])\n",
        "\n",
        "# Set callbacks.\n",
        "early_stopping = EarlyStopping(monitor='val_loss', verbose=1, patience=7, mode='min', restore_best_weights=True)\n",
        "callbacks = [early_stopping]\n",
        "\n",
        "# Conduct the desired train model\n",
        "model, test_metrics, test_generator = train_model(cfg, data, callbacks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aosIDzFgj9L",
        "outputId": "d52aaa36-4ba1-4653-8a53-17c8b88b905a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The generated file is covid.h5\n"
          ]
        }
      ],
      "source": [
        "# Generate h5 file\n",
        "model_path = 'covid.h5'\n",
        "save_model(model, model_path)  # Save the model\n",
        "\n",
        "print(\"The generated file is {}\".format(model_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLAbhpobaNIq"
      },
      "source": [
        "#Credits#\n",
        "\n",
        "1.   https://github.com/aildnont/covid-cxr\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "dnn_resnet.ipynb",
      "provenance": [],
      "background_execution": "on"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}